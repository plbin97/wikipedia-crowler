<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8"/>
<title>Language model - Wikipedia</title>
<script>document.documentElement.className = document.documentElement.className.replace( /(^|\s)client-nojs(\s|$)/, "$1client-js$2" );</script>
<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Language_model","wgTitle":"Language model","wgCurRevisionId":880427857,"wgRevisionId":880427857,"wgArticleId":1911810,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["All articles needing examples","Articles needing examples from December 2017","All articles with unsourced statements","Articles with unsourced statements from December 2017","Wikipedia articles needing clarification from November 2018","CS1 maint: Uses authors parameter","Language modeling","Statistical natural language processing","Markov models"],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"Language_model","wgRelevantArticleId":1911810,"wgRequestId":"XGHbaQpAAEQAAEQNod8AAACX","wgCSPNonce":false,"wgIsProbablyEditable":true,"wgRelevantPageIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgFlaggedRevsParams":{"tags":{}},"wgStableRevisionId":null,"wgCategoryTreePageCategoryOptions":"{\"mode\":0,\"hideprefix\":20,\"showcount\":true,\"namespaces\":false}","wgWikiEditorEnabledModules":[],"wgBetaFeaturesFeatures":[],"wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgPopupsShouldSendModuleToUser":true,"wgPopupsConflictsWithNavPopupGadget":false,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en","usePageImages":true,"usePageDescriptions":true},"wgMFIsPageContentModelEditable":true,"wgMFExpandAllSectionsUserOption":true,"wgMFEnableFontChanger":true,"wgMFDisplayWikibaseDescriptions":{"search":true,"nearby":true,"watchlist":true,"tagline":false},"wgRelatedArticles":null,"wgRelatedArticlesUseCirrusSearch":true,"wgRelatedArticlesOnlyUseCirrusSearch":false,"wgWMESchemaEditAttemptStepOversample":false,"wgPoweredByHHVM":true,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralNoticeCookiesToDelete":[],"wgCentralNoticeCategoriesUsingLegacy":["Fundraising","fundraising"],"wgWikibaseItemId":"Q3621696","wgScoreNoteLanguages":{"arabic":"العربية","catalan":"català","deutsch":"Deutsch","english":"English","espanol":"español","italiano":"italiano","nederlands":"Nederlands","norsk":"norsk","portugues":"português","suomi":"suomi","svenska":"svenska","vlaams":"West-Vlams"},"wgScoreDefaultNoteLanguage":"nederlands","wgCentralAuthMobileDomain":false,"wgCodeMirrorEnabled":true,"wgVisualEditorToolbarScrollOffset":0,"wgVisualEditorUnsupportedEditParams":["undo","undoafter","veswitched"],"wgEditSubmitButtonLabelPublish":true,"oresWikiId":"enwiki","oresBaseUrl":"http://ores.discovery.wmnet:8081/","oresApiVersion":3});mw.loader.state({"ext.gadget.charinsert-styles":"ready","ext.globalCssJs.user.styles":"ready","ext.globalCssJs.site.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","ext.globalCssJs.site":"ready","user":"ready","user.options":"ready","user.tokens":"loading","ext.math.styles":"ready","ext.cite.styles":"ready","mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready","mediawiki.toc.styles":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","ext.3d.styles":"ready","mediawiki.skinning.interface":"ready","skins.vector.styles":"ready"});mw.loader.implement("user.tokens@0tffind",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});RLPAGEMODULES=["ext.math.scripts","ext.cite.ux-enhancements","site","mediawiki.page.startup","mediawiki.page.ready","mediawiki.toc","mediawiki.searchSuggest","ext.gadget.teahouse","ext.gadget.ReferenceTooltips","ext.gadget.watchlist-notice","ext.gadget.DRN-wizard","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.centralauth.centralautologin","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.eventlogger","ext.uls.init","ext.uls.compactlinks","ext.uls.interface","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp","skins.vector.js"];mw.loader.load(RLPAGEMODULES);});</script>
<link rel="stylesheet" href="/w/load.php?debug=false&amp;lang=en&amp;modules=ext.3d.styles%7Cext.cite.styles%7Cext.math.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.skinning.interface%7Cmediawiki.toc.styles%7Cskins.vector.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector"/>
<script async="" src="/w/load.php?debug=false&amp;lang=en&amp;modules=startup&amp;only=scripts&amp;skin=vector"></script>
<meta name="ResourceLoaderDynamicStyles" content=""/>
<link rel="stylesheet" href="/w/load.php?debug=false&amp;lang=en&amp;modules=ext.gadget.charinsert-styles&amp;only=styles&amp;skin=vector"/>
<link rel="stylesheet" href="/w/load.php?debug=false&amp;lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector"/>
<meta name="generator" content="MediaWiki 1.33.0-wmf.16"/>
<meta name="referrer" content="origin"/>
<meta name="referrer" content="origin-when-crossorigin"/>
<meta name="referrer" content="origin-when-cross-origin"/>
<link rel="alternate" href="android-app://org.wikipedia/http/en.m.wikipedia.org/wiki/Language_model"/>
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Language_model&amp;action=edit"/>
<link rel="edit" title="Edit this page" href="/w/index.php?title=Language_model&amp;action=edit"/>
<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png"/>
<link rel="shortcut icon" href="/static/favicon/wikipedia.ico"/>
<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)"/>
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd"/>
<link rel="license" href="//creativecommons.org/licenses/by-sa/3.0/"/>
<link rel="canonical" href="https://en.wikipedia.org/wiki/Language_model"/>
<link rel="dns-prefetch" href="//login.wikimedia.org"/>
<link rel="dns-prefetch" href="//meta.wikimedia.org" />
<!--[if lt IE 9]><script src="/w/load.php?debug=false&amp;lang=en&amp;modules=html5shiv&amp;only=scripts&amp;skin=vector&amp;sync=1"></script><![endif]-->
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Language_model rootpage-Language_model skin-vector action-view">		<div id="mw-page-base" class="noprint"></div>
		<div id="mw-head-base" class="noprint"></div>
		<div id="content" class="mw-body" role="main">
			<a id="top"></a>
			<div id="siteNotice" class="mw-body-content"><!-- CentralNotice --></div><div class="mw-indicators mw-body-content">
</div>
<h1 id="firstHeading" class="firstHeading" lang="en">Language model</h1>			<div id="bodyContent" class="mw-body-content">
				<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>				<div id="contentSub"></div>
				<div id="jump-to-nav"></div>				<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
				<a class="mw-jump-link" href="#p-search">Jump to search</a>
				<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><div class="mw-parser-output"><p>A statistical <b>language model</b> is a <a href="/wiki/Probability_distribution" title="Probability distribution">probability distribution</a> over sequences of words. Given such a sequence, say of length <span class="texhtml mvar" style="font-style:italic;">m</span>, it assigns a probability <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle P(w_{1},\ldots ,w_{m})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>P</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <mo>&#x2026;<!-- … --></mo>
        <mo>,</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>m</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle P(w_{1},\ldots ,w_{m})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7a7d2debedc6711cdc40d6617617c0c9cc67e945" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:14.79ex; height:2.843ex;" alt="P(w_{1},\ldots ,w_{m})"/></span> to the whole sequence. Having a way to estimate the <a href="/wiki/Relative_likelihood" class="mw-redirect" title="Relative likelihood">relative likelihood</a> of different phrases is useful in many <a href="/wiki/Natural_language_processing" title="Natural language processing">natural language processing</a> applications, especially ones that generate text as an output. Language modeling is used in <a href="/wiki/Speech_recognition" title="Speech recognition">speech recognition</a>, <a href="/wiki/Machine_translation" title="Machine translation">machine translation</a>, <a href="/wiki/Part-of-speech_tagging" title="Part-of-speech tagging">part-of-speech tagging</a>, <a href="/wiki/Parsing" title="Parsing">parsing</a>, <a href="/wiki/Optical_Character_Recognition" class="mw-redirect" title="Optical Character Recognition">Optical Character Recognition</a>, <a href="/wiki/Handwriting_recognition" title="Handwriting recognition">handwriting recognition</a>, <a href="/wiki/Information_retrieval" title="Information retrieval">information retrieval</a> and other applications.
</p><p>In speech recognition, the computer tries to match sounds with word sequences. The language model provides context to distinguish between words and phrases that sound similar. For example, in <a href="/wiki/American_English" title="American English">American English</a>, the phrases "recognize speech" and "wreck a nice beach" are pronounced almost the same but mean very different things. These ambiguities are easier to resolve when evidence from the language model is incorporated with the pronunciation model and the <a href="/wiki/Acoustic_model" title="Acoustic model">acoustic model</a>.
</p><p>Language models are used in information retrieval in the <a href="/wiki/Query_likelihood_model" title="Query likelihood model">query likelihood model</a>. Here a separate language model is associated with each <a href="/wiki/Document" title="Document">document</a> in a collection. Documents are ranked based on the probability of the query <i>Q</i> in the document's language model <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle P(Q\mid M_{d})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>P</mi>
        <mo stretchy="false">(</mo>
        <mi>Q</mi>
        <mo>&#x2223;<!-- ∣ --></mo>
        <msub>
          <mi>M</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>d</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle P(Q\mid M_{d})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/184e6286edd58eebf6d40d9601ff4bf35ff540fd" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:10.676ex; height:2.843ex;" alt="P(Q\mid M_{d})"/></span>. Commonly, the <a href="/wiki/Unigram" class="mw-redirect" title="Unigram">unigram</a> language model is used for this purpose—otherwise known as the <a href="/wiki/Bag_of_words_model" class="mw-redirect" title="Bag of words model">bag of words model</a>.
</p><p>Data sparsity is a major problem in building language models. Most possible word sequences will not be observed in training. One solution is to make the assumption that the probability of a word only depends on the previous <i>n</i> words. This is known as an <a href="/wiki/N-gram" title="N-gram"><i>n</i>-gram</a> model or unigram model when <i>n</i>&#160;=&#160;1.
</p>
<div id="toc" class="toc"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none" /><div class="toctitle" lang="en" dir="ltr"><h2>Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Unigram_models"><span class="tocnumber">1</span> <span class="toctext">Unigram models</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#n-gram_models"><span class="tocnumber">2</span> <span class="toctext"><i>n</i>-gram models</span></a>
<ul>
<li class="toclevel-2 tocsection-3"><a href="#Example"><span class="tocnumber">2.1</span> <span class="toctext">Example</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-4"><a href="#Exponential_language_models"><span class="tocnumber">3</span> <span class="toctext">Exponential language models</span></a></li>
<li class="toclevel-1 tocsection-5"><a href="#Neural_language_models"><span class="tocnumber">4</span> <span class="toctext">Neural language models</span></a></li>
<li class="toclevel-1 tocsection-6"><a href="#Other_models"><span class="tocnumber">5</span> <span class="toctext">Other models</span></a></li>
<li class="toclevel-1 tocsection-7"><a href="#See_also"><span class="tocnumber">6</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-8"><a href="#Notes"><span class="tocnumber">7</span> <span class="toctext">Notes</span></a></li>
<li class="toclevel-1 tocsection-9"><a href="#References"><span class="tocnumber">8</span> <span class="toctext">References</span></a>
<ul>
<li class="toclevel-2 tocsection-10"><a href="#Citations"><span class="tocnumber">8.1</span> <span class="toctext">Citations</span></a></li>
<li class="toclevel-2 tocsection-11"><a href="#Sources"><span class="tocnumber">8.2</span> <span class="toctext">Sources</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-12"><a href="#External_links"><span class="tocnumber">9</span> <span class="toctext">External links</span></a></li>
</ul>
</div>

<h2><span class="mw-headline" id="Unigram_models">Unigram models</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Language_model&amp;action=edit&amp;section=1" title="Edit section: Unigram models">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>A unigram model used in information retrieval can be treated as the combination of several one-state <a href="/wiki/Finite-state_machine" title="Finite-state machine">finite automata</a>.<sup id="cite_ref-1" class="reference"><a href="#cite_note-1">&#91;1&#93;</a></sup> It splits the probabilities of different terms in a context, e.g. from <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle P(t_{1}t_{2}t_{3})=P(t_{1})P(t_{2}\mid t_{1})P(t_{3}\mid t_{1}t_{2})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>P</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>t</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <msub>
          <mi>t</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <msub>
          <mi>t</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>3</mn>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mi>P</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>t</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mi>P</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>t</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo>&#x2223;<!-- ∣ --></mo>
        <msub>
          <mi>t</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mi>P</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>t</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>3</mn>
          </mrow>
        </msub>
        <mo>&#x2223;<!-- ∣ --></mo>
        <msub>
          <mi>t</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <msub>
          <mi>t</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle P(t_{1}t_{2}t_{3})=P(t_{1})P(t_{2}\mid t_{1})P(t_{3}\mid t_{1}t_{2})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f355132c52e628006620c34cb67a2e49eda81719" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:38.236ex; height:2.843ex;" alt="P(t_{1}t_{2}t_{3})=P(t_{1})P(t_{2}\mid t_{1})P(t_{3}\mid t_{1}t_{2})"/></span> to <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle P_{\text{uni}}(t_{1}t_{2}t_{3})=P(t_{1})P(t_{2})P(t_{3})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>P</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mtext>uni</mtext>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>t</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <msub>
          <mi>t</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <msub>
          <mi>t</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>3</mn>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mi>P</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>t</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mi>P</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>t</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mi>P</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>t</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>3</mn>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle P_{\text{uni}}(t_{1}t_{2}t_{3})=P(t_{1})P(t_{2})P(t_{3})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/456c00c10da32de52b64899795311b6a49320579" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:30.945ex; height:2.843ex;" alt="P_{{\text{uni}}}(t_{1}t_{2}t_{3})=P(t_{1})P(t_{2})P(t_{3})"/></span>.
</p><p>In this model, the probability of each word only depends on that word's own probability in the document, so we only have one-state finite automata as units. The automaton itself has a probability distribution over the entire vocabulary of the model, summing to 1. The following is an illustration of a unigram model of a document.
</p>
<table class="wikitable">

<tbody><tr>
<th>Terms</th>
<th>Probability in doc
</th></tr>
<tr>
<td>a</td>
<td>0.1
</td></tr>
<tr>
<td>world</td>
<td>0.2
</td></tr>
<tr>
<td>likes</td>
<td>0.05
</td></tr>
<tr>
<td>we</td>
<td>0.05
</td></tr>
<tr>
<td>share</td>
<td>0.3
</td></tr>
<tr>
<td>...</td>
<td>...
</td></tr></tbody></table>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \sum _{\text{term in doc}}P({\text{term}})=1\,}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <munder>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mtext>term in doc</mtext>
          </mrow>
        </munder>
        <mi>P</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mtext>term</mtext>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mn>1</mn>
        <mspace width="thinmathspace" />
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \sum _{\text{term in doc}}P({\text{term}})=1\,}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/df049813f77140806d24a7a398d2c071a079ddad" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; width:21.756ex; height:5.509ex;" alt="\sum _{{{\text{term in doc}}}}P({\text{term}})=1\,"/></span></dd></dl>
<p>The probability generated for a specific query is calculated as
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle P({\text{query}})=\prod _{\text{term in query}}P({\text{term}})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>P</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mtext>query</mtext>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <munder>
          <mo>&#x220F;<!-- ∏ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mtext>term in query</mtext>
          </mrow>
        </munder>
        <mi>P</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mtext>term</mtext>
        </mrow>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle P({\text{query}})=\prod _{\text{term in query}}P({\text{term}})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cbc012a951394909b89862a4cbb9627f8c987f0c" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.338ex; width:31.011ex; height:5.843ex;" alt="P({\text{query}})=\prod _{{{\text{term in query}}}}P({\text{term}})"/></span></dd></dl>
<p>For different documents, we can build their own unigram models, with different hitting probabilities of words in it. And we use probabilities from different documents to generate different hitting probabilities for a query. Then we can rank documents for a query according to the generating probabilities. Next is an example of two unigram models of two documents.
</p>
<table class="wikitable">

<tbody><tr>
<th>Terms</th>
<th>Probability in Doc1</th>
<th>Probability in Doc2
</th></tr>
<tr>
<td>a</td>
<td>0.1</td>
<td>0.3
</td></tr>
<tr>
<td>world</td>
<td>0.2</td>
<td>0.1
</td></tr>
<tr>
<td>likes</td>
<td>0.05</td>
<td>0.03
</td></tr>
<tr>
<td>we</td>
<td>0.05</td>
<td>0.02
</td></tr>
<tr>
<td>share</td>
<td>0.3</td>
<td>0.2
</td></tr>
<tr>
<td>...</td>
<td>...</td>
<td>...
</td></tr></tbody></table>
<p>In information retrieval contexts, unigram language models are often smoothed to avoid instances where <i>P</i>(term)&#160;=&#160;0. A common approach is to generate a maximum-likelihood model for the entire collection and <a href="/wiki/Linear_interpolation" title="Linear interpolation">linearly interpolate</a> the collection model with a maximum-likelihood model for each document to create a smoothed document model.<sup id="cite_ref-2" class="reference"><a href="#cite_note-2">&#91;2&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="n-gram_models"><i>n</i>-gram models</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Language_model&amp;action=edit&amp;section=2" title="Edit section: n-gram models">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="/wiki/N-gram" title="N-gram">n-gram</a></div>
<p>In an <i>n</i>-gram model, the probability <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle P(w_{1},\ldots ,w_{m})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>P</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <mo>&#x2026;<!-- … --></mo>
        <mo>,</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>m</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle P(w_{1},\ldots ,w_{m})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7a7d2debedc6711cdc40d6617617c0c9cc67e945" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:14.79ex; height:2.843ex;" alt="P(w_{1},\ldots ,w_{m})"/></span> of observing the sentence <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle w_{1},\ldots ,w_{m}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <mo>&#x2026;<!-- … --></mo>
        <mo>,</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>m</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle w_{1},\ldots ,w_{m}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/26ab954b405d0aaa342c0053fc6e67dbb78dcf66" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:11.236ex; height:2.009ex;" alt="w_{1},\ldots ,w_{m}"/></span> is approximated as
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle P(w_{1},\ldots ,w_{m})=\prod _{i=1}^{m}P(w_{i}\mid w_{1},\ldots ,w_{i-1})\approx \prod _{i=1}^{m}P(w_{i}\mid w_{i-(n-1)},\ldots ,w_{i-1})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>P</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <mo>&#x2026;<!-- … --></mo>
        <mo>,</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>m</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <munderover>
          <mo>&#x220F;<!-- ∏ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>m</mi>
          </mrow>
        </munderover>
        <mi>P</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo>&#x2223;<!-- ∣ --></mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <mo>&#x2026;<!-- … --></mo>
        <mo>,</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>&#x2212;<!-- − --></mo>
            <mn>1</mn>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo>&#x2248;<!-- ≈ --></mo>
        <munderover>
          <mo>&#x220F;<!-- ∏ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>m</mi>
          </mrow>
        </munderover>
        <mi>P</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo>&#x2223;<!-- ∣ --></mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>&#x2212;<!-- − --></mo>
            <mo stretchy="false">(</mo>
            <mi>n</mi>
            <mo>&#x2212;<!-- − --></mo>
            <mn>1</mn>
            <mo stretchy="false">)</mo>
          </mrow>
        </msub>
        <mo>,</mo>
        <mo>&#x2026;<!-- … --></mo>
        <mo>,</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>&#x2212;<!-- − --></mo>
            <mn>1</mn>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle P(w_{1},\ldots ,w_{m})=\prod _{i=1}^{m}P(w_{i}\mid w_{1},\ldots ,w_{i-1})\approx \prod _{i=1}^{m}P(w_{i}\mid w_{i-(n-1)},\ldots ,w_{i-1})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9d95da0c97c84b21e1550eb2fb630981c63132f6" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; width:73.924ex; height:6.843ex;" alt="P(w_{1},\ldots ,w_{m})=\prod _{{i=1}}^{m}P(w_{i}\mid w_{1},\ldots ,w_{{i-1}})\approx \prod _{{i=1}}^{m}P(w_{i}\mid w_{{i-(n-1)}},\ldots ,w_{{i-1}})"/></span></dd></dl>
<p>Here, it is assumed that the probability of observing the <i>i<sup>th</sup></i> word <i>w<sub>i</sub></i> in the context history of the preceding <i>i</i>&#160;−&#160;1 words can be approximated by the probability of observing it in the shortened context history of the preceding <i>n</i>&#160;&#8722;&#160;1 words (<i>n</i><sup>th</sup> order <a href="/wiki/Markov_property" title="Markov property">Markov property</a>).
</p><p>The conditional probability can be calculated from <i>n</i>-gram model frequency counts:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle P(w_{i}\mid w_{i-(n-1)},\ldots ,w_{i-1})={\frac {\mathrm {count} (w_{i-(n-1)},\ldots ,w_{i-1},w_{i})}{\mathrm {count} (w_{i-(n-1)},\ldots ,w_{i-1})}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>P</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo>&#x2223;<!-- ∣ --></mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>&#x2212;<!-- − --></mo>
            <mo stretchy="false">(</mo>
            <mi>n</mi>
            <mo>&#x2212;<!-- − --></mo>
            <mn>1</mn>
            <mo stretchy="false">)</mo>
          </mrow>
        </msub>
        <mo>,</mo>
        <mo>&#x2026;<!-- … --></mo>
        <mo>,</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>&#x2212;<!-- − --></mo>
            <mn>1</mn>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mrow class="MJX-TeXAtom-ORD">
                <mi mathvariant="normal">c</mi>
                <mi mathvariant="normal">o</mi>
                <mi mathvariant="normal">u</mi>
                <mi mathvariant="normal">n</mi>
                <mi mathvariant="normal">t</mi>
              </mrow>
              <mo stretchy="false">(</mo>
              <msub>
                <mi>w</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                  <mo>&#x2212;<!-- − --></mo>
                  <mo stretchy="false">(</mo>
                  <mi>n</mi>
                  <mo>&#x2212;<!-- − --></mo>
                  <mn>1</mn>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msub>
              <mo>,</mo>
              <mo>&#x2026;<!-- … --></mo>
              <mo>,</mo>
              <msub>
                <mi>w</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                  <mo>&#x2212;<!-- − --></mo>
                  <mn>1</mn>
                </mrow>
              </msub>
              <mo>,</mo>
              <msub>
                <mi>w</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                </mrow>
              </msub>
              <mo stretchy="false">)</mo>
            </mrow>
            <mrow>
              <mrow class="MJX-TeXAtom-ORD">
                <mi mathvariant="normal">c</mi>
                <mi mathvariant="normal">o</mi>
                <mi mathvariant="normal">u</mi>
                <mi mathvariant="normal">n</mi>
                <mi mathvariant="normal">t</mi>
              </mrow>
              <mo stretchy="false">(</mo>
              <msub>
                <mi>w</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                  <mo>&#x2212;<!-- − --></mo>
                  <mo stretchy="false">(</mo>
                  <mi>n</mi>
                  <mo>&#x2212;<!-- − --></mo>
                  <mn>1</mn>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msub>
              <mo>,</mo>
              <mo>&#x2026;<!-- … --></mo>
              <mo>,</mo>
              <msub>
                <mi>w</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                  <mo>&#x2212;<!-- − --></mo>
                  <mn>1</mn>
                </mrow>
              </msub>
              <mo stretchy="false">)</mo>
            </mrow>
          </mfrac>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle P(w_{i}\mid w_{i-(n-1)},\ldots ,w_{i-1})={\frac {\mathrm {count} (w_{i-(n-1)},\ldots ,w_{i-1},w_{i})}{\mathrm {count} (w_{i-(n-1)},\ldots ,w_{i-1})}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/566e849e4736f67494016225a13a93f9ce273f07" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -2.838ex; width:58.584ex; height:6.843ex;" alt="P(w_{i}\mid w_{{i-(n-1)}},\ldots ,w_{{i-1}})={\frac  {{\mathrm  {count}}(w_{{i-(n-1)}},\ldots ,w_{{i-1}},w_{i})}{{\mathrm  {count}}(w_{{i-(n-1)}},\ldots ,w_{{i-1}})}}"/></span></dd></dl>
<p>The words <b>bigram</b> and <b>trigram</b> language model denote <i>n</i>-gram model language models with <i>n</i>&#160;=&#160;2 and <i>n</i>&#160;=&#160;3, respectively.<sup id="cite_ref-3" class="reference"><a href="#cite_note-3">&#91;3&#93;</a></sup>
</p><p>Typically, however, the <i>n</i>-gram model probabilities are not derived directly from the frequency counts, because models derived this way have severe problems when confronted with any <i>n</i>-grams that have not explicitly been seen before.  Instead, some form of <i>smoothing</i> is necessary, assigning some of the total probability mass to unseen words or <i>n</i>-grams. Various methods are used, from simple "add-one" smoothing (assign a count of 1 to unseen <i>n</i>-grams, as an <a href="/wiki/Uninformative_prior" class="mw-redirect" title="Uninformative prior">uninformative prior</a>) to more sophisticated models, such as <a href="/wiki/Good-Turing_discounting" class="mw-redirect" title="Good-Turing discounting">Good-Turing discounting</a> or <a href="/wiki/Katz%27s_back-off_model" title="Katz&#39;s back-off model">back-off models</a>.
</p>
<h3><span class="mw-headline" id="Example">Example</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Language_model&amp;action=edit&amp;section=3" title="Edit section: Example">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>In a bigram (<i>n</i>&#160;=&#160;2) language model, the probability of the sentence <i>I saw the red house</i> is approximated as
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\begin{aligned}&amp;P({\text{I, saw, the, red, house}})\\\approx {}&amp;P({\text{I}}\mid \langle s\rangle )P({\text{saw}}\mid {\text{I}})P({\text{the}}\mid {\text{saw}})P({\text{red}}\mid {\text{the}})P({\text{house}}\mid {\text{red}})P(\langle /s\rangle \mid {\text{house}})\end{aligned}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true">
            <mtr>
              <mtd />
              <mtd>
                <mi>P</mi>
                <mo stretchy="false">(</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>I, saw, the, red, house</mtext>
                </mrow>
                <mo stretchy="false">)</mo>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mo>&#x2248;<!-- ≈ --></mo>
                <mrow class="MJX-TeXAtom-ORD">

                </mrow>
              </mtd>
              <mtd>
                <mi>P</mi>
                <mo stretchy="false">(</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>I</mtext>
                </mrow>
                <mo>&#x2223;<!-- ∣ --></mo>
                <mo fence="false" stretchy="false">&#x27E8;<!-- ⟨ --></mo>
                <mi>s</mi>
                <mo fence="false" stretchy="false">&#x27E9;<!-- ⟩ --></mo>
                <mo stretchy="false">)</mo>
                <mi>P</mi>
                <mo stretchy="false">(</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>saw</mtext>
                </mrow>
                <mo>&#x2223;<!-- ∣ --></mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>I</mtext>
                </mrow>
                <mo stretchy="false">)</mo>
                <mi>P</mi>
                <mo stretchy="false">(</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>the</mtext>
                </mrow>
                <mo>&#x2223;<!-- ∣ --></mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>saw</mtext>
                </mrow>
                <mo stretchy="false">)</mo>
                <mi>P</mi>
                <mo stretchy="false">(</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>red</mtext>
                </mrow>
                <mo>&#x2223;<!-- ∣ --></mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>the</mtext>
                </mrow>
                <mo stretchy="false">)</mo>
                <mi>P</mi>
                <mo stretchy="false">(</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>house</mtext>
                </mrow>
                <mo>&#x2223;<!-- ∣ --></mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>red</mtext>
                </mrow>
                <mo stretchy="false">)</mo>
                <mi>P</mi>
                <mo stretchy="false">(</mo>
                <mo fence="false" stretchy="false">&#x27E8;<!-- ⟨ --></mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo>/</mo>
                </mrow>
                <mi>s</mi>
                <mo fence="false" stretchy="false">&#x27E9;<!-- ⟩ --></mo>
                <mo>&#x2223;<!-- ∣ --></mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>house</mtext>
                </mrow>
                <mo stretchy="false">)</mo>
              </mtd>
            </mtr>
          </mtable>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\begin{aligned}&amp;P({\text{I, saw, the, red, house}})\\\approx {}&amp;P({\text{I}}\mid \langle s\rangle )P({\text{saw}}\mid {\text{I}})P({\text{the}}\mid {\text{saw}})P({\text{red}}\mid {\text{the}})P({\text{house}}\mid {\text{red}})P(\langle /s\rangle \mid {\text{house}})\end{aligned}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4b713a4cbfc138166dca5814a3a95f1bc5cb2069" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -2.505ex; width:76.635ex; height:6.176ex;" alt="{\begin{aligned}&amp;P({\text{I, saw, the, red, house}})\\\approx {}&amp;P({\text{I}}\mid \langle s\rangle )P({\text{saw}}\mid {\text{I}})P({\text{the}}\mid {\text{saw}})P({\text{red}}\mid {\text{the}})P({\text{house}}\mid {\text{red}})P(\langle /s\rangle \mid {\text{house}})\end{aligned}}"/></span></dd></dl>
<p>whereas in a trigram (<i>n</i>&#160;=&#160;3) language model, the approximation is
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\begin{aligned}&amp;P({\text{I, saw, the, red, house}})\\\approx {}&amp;P({\text{I}}\mid \langle s\rangle ,\langle s\rangle )P({\text{saw}}\mid \langle s\rangle ,I)P({\text{the}}\mid {\text{I, saw}})P({\text{red}}\mid {\text{saw, the}})P({\text{house}}\mid {\text{the, red}})P(\langle /s\rangle \mid {\text{red, house}})\end{aligned}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true">
            <mtr>
              <mtd />
              <mtd>
                <mi>P</mi>
                <mo stretchy="false">(</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>I, saw, the, red, house</mtext>
                </mrow>
                <mo stretchy="false">)</mo>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mo>&#x2248;<!-- ≈ --></mo>
                <mrow class="MJX-TeXAtom-ORD">

                </mrow>
              </mtd>
              <mtd>
                <mi>P</mi>
                <mo stretchy="false">(</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>I</mtext>
                </mrow>
                <mo>&#x2223;<!-- ∣ --></mo>
                <mo fence="false" stretchy="false">&#x27E8;<!-- ⟨ --></mo>
                <mi>s</mi>
                <mo fence="false" stretchy="false">&#x27E9;<!-- ⟩ --></mo>
                <mo>,</mo>
                <mo fence="false" stretchy="false">&#x27E8;<!-- ⟨ --></mo>
                <mi>s</mi>
                <mo fence="false" stretchy="false">&#x27E9;<!-- ⟩ --></mo>
                <mo stretchy="false">)</mo>
                <mi>P</mi>
                <mo stretchy="false">(</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>saw</mtext>
                </mrow>
                <mo>&#x2223;<!-- ∣ --></mo>
                <mo fence="false" stretchy="false">&#x27E8;<!-- ⟨ --></mo>
                <mi>s</mi>
                <mo fence="false" stretchy="false">&#x27E9;<!-- ⟩ --></mo>
                <mo>,</mo>
                <mi>I</mi>
                <mo stretchy="false">)</mo>
                <mi>P</mi>
                <mo stretchy="false">(</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>the</mtext>
                </mrow>
                <mo>&#x2223;<!-- ∣ --></mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>I, saw</mtext>
                </mrow>
                <mo stretchy="false">)</mo>
                <mi>P</mi>
                <mo stretchy="false">(</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>red</mtext>
                </mrow>
                <mo>&#x2223;<!-- ∣ --></mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>saw, the</mtext>
                </mrow>
                <mo stretchy="false">)</mo>
                <mi>P</mi>
                <mo stretchy="false">(</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>house</mtext>
                </mrow>
                <mo>&#x2223;<!-- ∣ --></mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>the, red</mtext>
                </mrow>
                <mo stretchy="false">)</mo>
                <mi>P</mi>
                <mo stretchy="false">(</mo>
                <mo fence="false" stretchy="false">&#x27E8;<!-- ⟨ --></mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo>/</mo>
                </mrow>
                <mi>s</mi>
                <mo fence="false" stretchy="false">&#x27E9;<!-- ⟩ --></mo>
                <mo>&#x2223;<!-- ∣ --></mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>red, house</mtext>
                </mrow>
                <mo stretchy="false">)</mo>
              </mtd>
            </mtr>
          </mtable>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\begin{aligned}&amp;P({\text{I, saw, the, red, house}})\\\approx {}&amp;P({\text{I}}\mid \langle s\rangle ,\langle s\rangle )P({\text{saw}}\mid \langle s\rangle ,I)P({\text{the}}\mid {\text{I, saw}})P({\text{red}}\mid {\text{saw, the}})P({\text{house}}\mid {\text{the, red}})P(\langle /s\rangle \mid {\text{red, house}})\end{aligned}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3b612bf3e90597b0ff06110cd424c03af085bfd6" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -2.505ex; width:100.807ex; height:6.176ex;" alt="{\begin{aligned}&amp;P({\text{I, saw, the, red, house}})\\\approx {}&amp;P({\text{I}}\mid \langle s\rangle ,\langle s\rangle )P({\text{saw}}\mid \langle s\rangle ,I)P({\text{the}}\mid {\text{I, saw}})P({\text{red}}\mid {\text{saw, the}})P({\text{house}}\mid {\text{the, red}})P(\langle /s\rangle \mid {\text{red, house}})\end{aligned}}"/></span></dd></dl>
<p>Note that the context of the first <i>n</i>&#160;–&#160;1 <i>n</i>-grams is filled with start-of-sentence markers, typically denoted &lt;s&gt;.
</p><p>Additionally, without an end-of-sentence marker, the probability of an ungrammatical sequence <i>*I saw the</i> would always be higher than that of the longer sentence <i>I saw the red house.</i>
</p>
<h2><span class="mw-headline" id="Exponential_language_models">Exponential language models</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Language_model&amp;action=edit&amp;section=4" title="Edit section: Exponential language models">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p><a href="/wiki/Principle_of_maximum_entropy" title="Principle of maximum entropy">Maximum entropy</a> language models encode the relationship between a word and the n-gram history using feature functions.
The equation is <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle P(w_{1},\ldots ,w_{m})={\frac {1}{Z(w_{1},\ldots ,w_{m-1})}}\exp(a^{T}f(w_{1},\ldots ,w_{m}))}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>P</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <mo>&#x2026;<!-- … --></mo>
        <mo>,</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>m</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mn>1</mn>
            <mrow>
              <mi>Z</mi>
              <mo stretchy="false">(</mo>
              <msub>
                <mi>w</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mn>1</mn>
                </mrow>
              </msub>
              <mo>,</mo>
              <mo>&#x2026;<!-- … --></mo>
              <mo>,</mo>
              <msub>
                <mi>w</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>m</mi>
                  <mo>&#x2212;<!-- − --></mo>
                  <mn>1</mn>
                </mrow>
              </msub>
              <mo stretchy="false">)</mo>
            </mrow>
          </mfrac>
        </mrow>
        <mi>exp</mi>
        <mo>&#x2061;<!-- ⁡ --></mo>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>a</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>T</mi>
          </mrow>
        </msup>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <mo>&#x2026;<!-- … --></mo>
        <mo>,</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>m</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle P(w_{1},\ldots ,w_{m})={\frac {1}{Z(w_{1},\ldots ,w_{m-1})}}\exp(a^{T}f(w_{1},\ldots ,w_{m}))}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/42ac853d50158cd7983e52f6d221620fe3b996e8" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -2.671ex; width:58.243ex; height:6.009ex;" alt="{\displaystyle P(w_{1},\ldots ,w_{m})={\frac {1}{Z(w_{1},\ldots ,w_{m-1})}}\exp(a^{T}f(w_{1},\ldots ,w_{m}))}"/></span> where 
<span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle Z(w_{1},\ldots ,w_{m-1})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>Z</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <mo>&#x2026;<!-- … --></mo>
        <mo>,</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>m</mi>
            <mo>&#x2212;<!-- − --></mo>
            <mn>1</mn>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle Z(w_{1},\ldots ,w_{m-1})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/db345c7489ff361147f1363116ee351dc88b5719" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:16.826ex; height:2.843ex;" alt="{\displaystyle Z(w_{1},\ldots ,w_{m-1})}"/></span> is the <a href="/wiki/Partition_function_(mathematics)" title="Partition function (mathematics)">partition function</a>, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle a}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>a</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle a}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ffd2487510aa438433a2579450ab2b3d557e5edc" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.23ex; height:1.676ex;" alt="a"/></span> is the 
parameter vector, and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle f(w_{1},\ldots ,w_{m})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <mo>&#x2026;<!-- … --></mo>
        <mo>,</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>m</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f(w_{1},\ldots ,w_{m})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/899a4aa9d383560f5fe3446a9ac9693c225bb184" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:14.324ex; height:2.843ex;" alt="{\displaystyle f(w_{1},\ldots ,w_{m})}"/></span> is the feature function. In the simplest case, the feature function
is just an indicator of the presence of a certain n-gram. It is helpful to use a prior on <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle a}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>a</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle a}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ffd2487510aa438433a2579450ab2b3d557e5edc" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.23ex; height:1.676ex;" alt="a"/></span> or some form of
regularization.
</p><p>The log-bilinear model is another example of an exponential language model.
</p>
<h2><span class="mw-headline" id="Neural_language_models">Neural language models</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Language_model&amp;action=edit&amp;section=5" title="Edit section: Neural language models">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Neural language models (or <i>Continuous space language models</i>) use continuous representations or embeddings of words to make their predictions.<sup id="cite_ref-4" class="reference"><a href="#cite_note-4">&#91;4&#93;</a></sup> These models make use of <a href="/wiki/Artificial_neural_network" title="Artificial neural network">Neural networks</a>. 
</p><p>Continuous space embeddings help to alleviate the <a href="/wiki/Curse_of_dimensionality" title="Curse of dimensionality">curse of dimensionality</a> in language modeling: as language models are trained on larger and larger texts, the number of unique words (the vocabulary) increases<sup id="cite_ref-5" class="reference"><a href="#cite_note-5">&#91;a&#93;</a></sup> and the number of possible sequences of words increases <a href="/wiki/Exponential_growth" title="Exponential growth">exponentially</a> with the size of the vocabulary, causing a data sparsity problem because for each of the exponentially many sequences. Thus, statistics are needed to properly estimate probabilities. Neural networks avoid this problem by representing words in a <a href="/wiki/Distributed_representation" class="mw-redirect" title="Distributed representation">distributed</a> way, as non-linear combinations of weights in a neural net.<sup id="cite_ref-bengio_6-0" class="reference"><a href="#cite_note-bengio-6">&#91;5&#93;</a></sup> An alternate description is that a neural net approximates the language function. The neural net architecture might be feed-forward or <a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">recurrent</a>, and while the former is simpler the latter is more common.<sup class="noprint Inline-Template" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:AUDIENCE" class="mw-redirect" title="Wikipedia:AUDIENCE"><span title="An editor has requested that an example be provided. (December 2017)">example  needed</span></a></i>&#93;</sup><sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (December 2017)">citation needed</span></a></i>&#93;</sup>
</p><p>Typically, neural net language models are constructed and trained as <a href="/wiki/Probabilistic_classifier" class="mw-redirect" title="Probabilistic classifier">probabilistic classifiers</a> that learn to predict a probability distribution
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle P(w_{t}|\mathrm {context} )\,\forall t\in V}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>P</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
        </msub>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">c</mi>
          <mi mathvariant="normal">o</mi>
          <mi mathvariant="normal">n</mi>
          <mi mathvariant="normal">t</mi>
          <mi mathvariant="normal">e</mi>
          <mi mathvariant="normal">x</mi>
          <mi mathvariant="normal">t</mi>
        </mrow>
        <mo stretchy="false">)</mo>
        <mspace width="thinmathspace" />
        <mi mathvariant="normal">&#x2200;<!-- ∀ --></mi>
        <mi>t</mi>
        <mo>&#x2208;<!-- ∈ --></mo>
        <mi>V</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle P(w_{t}|\mathrm {context} )\,\forall t\in V}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4925033c078a9cc814e50415e3f247f238e9ef79" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:21.395ex; height:2.843ex;" alt="P(w_{t}|{\mathrm  {context}})\,\forall t\in V"/></span>.</dd></dl>
<p>I.e., the network is trained to predict a probability distribution over the vocabulary, given some linguistic context. This is done using standard neural net training algorithms such as <a href="/wiki/Stochastic_gradient_descent" title="Stochastic gradient descent">stochastic gradient descent</a> with <a href="/wiki/Backpropagation" title="Backpropagation">backpropagation</a>.<sup id="cite_ref-bengio_6-1" class="reference"><a href="#cite_note-bengio-6">&#91;5&#93;</a></sup> The context might be a fixed-size window of previous words, so that the network predicts
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle P(w_{t}|w_{t-k},\dots ,w_{t-1})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>P</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
        </msub>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
            <mo>&#x2212;<!-- − --></mo>
            <mi>k</mi>
          </mrow>
        </msub>
        <mo>,</mo>
        <mo>&#x2026;<!-- … --></mo>
        <mo>,</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
            <mo>&#x2212;<!-- − --></mo>
            <mn>1</mn>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle P(w_{t}|w_{t-k},\dots ,w_{t-1})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/604ed33f04866f779b0c7f56ebab45b5fb50b3ca" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:21.086ex; height:2.843ex;" alt="P(w_{t}|w_{{t-k}},\dots ,w_{{t-1}})"/></span></dd></dl>
<p>from a <a href="/wiki/Feature_vector" class="mw-redirect" title="Feature vector">feature vector</a> representing the previous <span class="texhtml mvar" style="font-style:italic;">k</span> words.<sup id="cite_ref-bengio_6-2" class="reference"><a href="#cite_note-bengio-6">&#91;5&#93;</a></sup> Another option is to use "future" words as well as "past" words as features, so that the estimated probability is<sup id="cite_ref-mikolov_7-0" class="reference"><a href="#cite_note-mikolov-7">&#91;6&#93;</a></sup>
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle P(w_{t}|w_{t-k},\dots ,w_{t-1},w_{t+1},\dots ,w_{t+k})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>P</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
        </msub>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
            <mo>&#x2212;<!-- − --></mo>
            <mi>k</mi>
          </mrow>
        </msub>
        <mo>,</mo>
        <mo>&#x2026;<!-- … --></mo>
        <mo>,</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
            <mo>&#x2212;<!-- − --></mo>
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
            <mo>+</mo>
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <mo>&#x2026;<!-- … --></mo>
        <mo>,</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
            <mo>+</mo>
            <mi>k</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle P(w_{t}|w_{t-k},\dots ,w_{t-1},w_{t+1},\dots ,w_{t+k})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/da4cfcdcec06e213452b567c219c07257fe77ccd" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:36.513ex; height:2.843ex;" alt="P(w_{t}|w_{{t-k}},\dots ,w_{{t-1}},w_{{t+1}},\dots ,w_{{t+k}})"/></span>.</dd></dl>
<p>A third option, that allows faster training, is to invert the previous problem and make a neural network learn the context, given a word. One then maximizes the log-probability<sup id="cite_ref-compositionality_8-0" class="reference"><a href="#cite_note-compositionality-8">&#91;7&#93;</a></sup>
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \sum _{-k\leq j-1,\,j\leq k}\log P(w_{t+j}|w_{t})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <munder>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2212;<!-- − --></mo>
            <mi>k</mi>
            <mo>&#x2264;<!-- ≤ --></mo>
            <mi>j</mi>
            <mo>&#x2212;<!-- − --></mo>
            <mn>1</mn>
            <mo>,</mo>
            <mspace width="thinmathspace" />
            <mi>j</mi>
            <mo>&#x2264;<!-- ≤ --></mo>
            <mi>k</mi>
          </mrow>
        </munder>
        <mi>log</mi>
        <mo>&#x2061;<!-- ⁡ --></mo>
        <mi>P</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
            <mo>+</mo>
            <mi>j</mi>
          </mrow>
        </msub>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \sum _{-k\leq j-1,\,j\leq k}\log P(w_{t+j}|w_{t})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/84453537eecaaf21866832e5ffb118cd1c6f8fe7" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.338ex; width:24.732ex; height:5.843ex;" alt="\sum _{{-k\leq j-1,\,j\leq k}}\log P(w_{{t+j}}|w_{t})"/></span><sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="what is t? (November 2018)">clarification needed</span></a></i>&#93;</sup></dd></dl>
<p>This is called a <a href="/wiki/Skip-gram" class="mw-redirect" title="Skip-gram">skip-gram</a> language model, and is the basis of the popular<sup id="cite_ref-9" class="reference"><a href="#cite_note-9">&#91;8&#93;</a></sup> <a href="/wiki/Word2vec" title="Word2vec">word2vec</a> program.
</p><p>Instead of using neural net language models to produce actual probabilities, it is common to instead use the distributed representation encoded in the networks' "hidden" layers as representations of words; each word is then mapped onto an <span class="texhtml mvar" style="font-style:italic;">n</span>-dimensional real vector called the <a href="/wiki/Word_embedding" title="Word embedding">word embedding</a>, where <span class="texhtml mvar" style="font-style:italic;">n</span> is the size of the layer just before the output layer. The representations in skip-gram models have the distinct characteristic that they model semantic relations between words as <a href="/wiki/Linear_combination" title="Linear combination">linear combinations</a>, capturing a form of <a href="/wiki/Compositionality" class="mw-redirect" title="Compositionality">compositionality</a>. For example, in some such models, if <span class="texhtml mvar" style="font-style:italic;">v</span> is the function that maps a word <span class="texhtml mvar" style="font-style:italic;">w</span> to its <span class="texhtml mvar" style="font-style:italic;">n</span>-d vector representation, then
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle v(\mathrm {king} )-v(\mathrm {male} )+v(\mathrm {female} )\approx v(\mathrm {queen} )}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>v</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">k</mi>
          <mi mathvariant="normal">i</mi>
          <mi mathvariant="normal">n</mi>
          <mi mathvariant="normal">g</mi>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>&#x2212;<!-- − --></mo>
        <mi>v</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">m</mi>
          <mi mathvariant="normal">a</mi>
          <mi mathvariant="normal">l</mi>
          <mi mathvariant="normal">e</mi>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>+</mo>
        <mi>v</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">f</mi>
          <mi mathvariant="normal">e</mi>
          <mi mathvariant="normal">m</mi>
          <mi mathvariant="normal">a</mi>
          <mi mathvariant="normal">l</mi>
          <mi mathvariant="normal">e</mi>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>&#x2248;<!-- ≈ --></mo>
        <mi>v</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">q</mi>
          <mi mathvariant="normal">u</mi>
          <mi mathvariant="normal">e</mi>
          <mi mathvariant="normal">e</mi>
          <mi mathvariant="normal">n</mi>
        </mrow>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle v(\mathrm {king} )-v(\mathrm {male} )+v(\mathrm {female} )\approx v(\mathrm {queen} )}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8b2186a8c7c082260c363b8c16677f39fedaba00" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:42.202ex; height:2.843ex;" alt="v({\mathrm  {king}})-v({\mathrm  {male}})+v({\mathrm  {female}})\approx v({\mathrm  {queen}})"/></span></dd></dl>
<p>where ≈ is made precise by stipulating that its right-hand side must be the <a href="/wiki/Nearest_neighbor_search" title="Nearest neighbor search">nearest neighbor</a> of the value of the left-hand side.<sup id="cite_ref-mikolov_7-1" class="reference"><a href="#cite_note-mikolov-7">&#91;6&#93;</a></sup><sup id="cite_ref-compositionality_8-1" class="reference"><a href="#cite_note-compositionality-8">&#91;7&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Other_models">Other models</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Language_model&amp;action=edit&amp;section=6" title="Edit section: Other models">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>A <a href="/w/index.php?title=Positional_language_model&amp;action=edit&amp;redlink=1" class="new" title="Positional language model (page does not exist)">positional language model</a><sup id="cite_ref-10" class="reference"><a href="#cite_note-10">&#91;9&#93;</a></sup> is one that describes the probability of given words occurring close to one another in a text, not necessarily immediately adjacent. Similarly, bag-of-concepts models<sup id="cite_ref-11" class="reference"><a href="#cite_note-11">&#91;10&#93;</a></sup> leverage on the semantics associated with multi-word expressions such as <i>buy_christmas_present</i>, even when they are used in information-rich sentences like "today I bought a lot of very nice Christmas presents".
</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Language_model&amp;action=edit&amp;section=7" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a href="/wiki/Statistical_model" title="Statistical model">Statistical model</a></li>
<li><a href="/wiki/Factored_language_model" title="Factored language model">Factored language model</a></li>
<li><a href="/wiki/Cache_language_model" title="Cache language model">Cache language model</a></li>
<li><a href="/wiki/Katz%27s_back-off_model" title="Katz&#39;s back-off model">Katz's back-off model</a></li></ul>
<h2><span class="mw-headline" id="Notes">Notes</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Language_model&amp;action=edit&amp;section=8" title="Edit section: Notes">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist" style="list-style-type: lower-alpha;">
<div class="mw-references-wrap"><ol class="references">
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text">See <a href="/wiki/Heaps%27_law" title="Heaps&#39; law">Heaps' law</a>.</span>
</li>
</ol></div></div>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Language_model&amp;action=edit&amp;section=9" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Citations">Citations</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Language_model&amp;action=edit&amp;section=10" title="Edit section: Citations">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="reflist" style="list-style-type: decimal;">
<div class="mw-references-wrap"><ol class="references">
<li id="cite_note-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-1">^</a></b></span> <span class="reference-text">Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze: An Introduction to Information Retrieval, pages 237–240. Cambridge University Press, 2009</span>
</li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text">Buttcher, Clarke, and Cormack. Information Retrieval: Implementing and Evaluating Search Engines. pg. 289–291. MIT Press.</span>
</li>
<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3">^</a></b></span> <span class="reference-text">Craig Trim, <a rel="nofollow" class="external text" href="http://trimc-nlp.blogspot.com/2013/04/language-modeling.html"><i>What is Language Modeling?</i></a>, April 26th, 2013.</span>
</li>
<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text"><cite class="citation web">Karpathy, Andrej. <a rel="nofollow" class="external text" href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">"The Unreasonable Effectiveness of Recurrent Neural Networks"</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=The+Unreasonable+Effectiveness+of+Recurrent+Neural+Networks&amp;rft.aulast=Karpathy&amp;rft.aufirst=Andrej&amp;rft_id=https%3A%2F%2Fkarpathy.github.io%2F2015%2F05%2F21%2Frnn-effectiveness%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALanguage+model" class="Z3988"></span><style data-mw-deduplicate="TemplateStyles:r879151008">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}</style></span>
</li>
<li id="cite_note-bengio-6"><span class="mw-cite-backlink">^ <a href="#cite_ref-bengio_6-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-bengio_6-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-bengio_6-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation encyclopaedia">Bengio, Yoshua (2008). <a rel="nofollow" class="external text" href="http://www.scholarpedia.org/article/Neural_net_language_models">"Neural net language models"</a>. <i><a href="/wiki/Scholarpedia" title="Scholarpedia">Scholarpedia</a></i>. <b>3</b>. p.&#160;3881.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Neural+net+language+models&amp;rft.btitle=Scholarpedia&amp;rft.pages=3881&amp;rft.date=2008&amp;rft.aulast=Bengio&amp;rft.aufirst=Yoshua&amp;rft_id=http%3A%2F%2Fwww.scholarpedia.org%2Farticle%2FNeural_net_language_models&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALanguage+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r879151008"/></span>
</li>
<li id="cite_note-mikolov-7"><span class="mw-cite-backlink">^ <a href="#cite_ref-mikolov_7-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-mikolov_7-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation arxiv">Mikolov, Tomas; Chen, Kai; Corrado, Greg; Dean, Jeffrey (2013). "Efficient estimation of word representations in vector space". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1301.3781">1301.3781</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Efficient+estimation+of+word+representations+in+vector+space&amp;rft.date=2013&amp;rft_id=info%3Aarxiv%2F1301.3781&amp;rft.aulast=Mikolov&amp;rft.aufirst=Tomas&amp;rft.au=Chen%2C+Kai&amp;rft.au=Corrado%2C+Greg&amp;rft.au=Dean%2C+Jeffrey&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALanguage+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r879151008"/></span>
</li>
<li id="cite_note-compositionality-8"><span class="mw-cite-backlink">^ <a href="#cite_ref-compositionality_8-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-compositionality_8-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation conference">Mikolov, Tomas; Sutskever, Ilya; Chen, Kai; Corrado irst4=Greg S.; Dean, Jeff (2013). <a rel="nofollow" class="external text" href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"><i>Distributed Representations of Words and Phrases and their Compositionality</i></a> <span class="cs1-format">(PDF)</span>. <a href="/wiki/Advances_in_Neural_Information_Processing_Systems" class="mw-redirect" title="Advances in Neural Information Processing Systems">Advances in Neural Information Processing Systems</a>. pp.&#160;3111–3119.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Distributed+Representations+of+Words+and+Phrases+and+their+Compositionality&amp;rft.pages=3111-3119&amp;rft.date=2013&amp;rft.aulast=Mikolov&amp;rft.aufirst=Tomas&amp;rft.au=Sutskever%2C+Ilya&amp;rft.au=Chen%2C+Kai&amp;rft.au=Corrado+irst4%3DGreg+S.&amp;rft.au=Dean%2C+Jeff&amp;rft_id=http%3A%2F%2Fpapers.nips.cc%2Fpaper%2F5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALanguage+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r879151008"/></span>
</li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text"><cite class="citation web">Harris, Derrick (16 August 2013). <a rel="nofollow" class="external text" href="https://gigaom.com/2013/08/16/were-on-the-cusp-of-deep-learning-for-the-masses-you-can-thank-google-later/">"We're on the cusp of deep learning for the masses. You can thank Google later"</a>. <i>Gigaom</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Gigaom&amp;rft.atitle=We%E2%80%99re+on+the+cusp+of+deep+learning+for+the+masses.+You+can+thank+Google+later&amp;rft.date=2013-08-16&amp;rft.aulast=Harris&amp;rft.aufirst=Derrick&amp;rft_id=https%3A%2F%2Fgigaom.com%2F2013%2F08%2F16%2Fwere-on-the-cusp-of-deep-learning-for-the-masses-you-can-thank-google-later%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALanguage+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r879151008"/></span>
</li>
<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text">Yuanhua Lv and ChengXiang Zhai, <a rel="nofollow" class="external text" href="http://times.cs.uiuc.edu/czhai/pub/sigir09-PLM.pdf"><i>Positional Language Models for Information Retrieval</i></a>, in Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval (SIGIR), 2009.</span>
</li>
<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11">^</a></b></span> <span class="reference-text">E. Cambria and A. Hussain. Sentic Computing: Techniques, Tools, and Applications. Dordrecht, Netherlands: Springer, <link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r879151008"/><a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-94-007-5069-2" title="Special:BookSources/978-94-007-5069-2">978-94-007-5069-2</a> (2012)</span>
</li>
</ol></div></div>
<h3><span class="mw-headline" id="Sources">Sources</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Language_model&amp;action=edit&amp;section=11" title="Edit section: Sources">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<style data-mw-deduplicate="TemplateStyles:r853264625">.mw-parser-output .refbegin{font-size:90%;margin-bottom:0.5em}.mw-parser-output .refbegin-hanging-indents>ul{list-style-type:none;margin-left:0}.mw-parser-output .refbegin-hanging-indents>ul>li,.mw-parser-output .refbegin-hanging-indents>dl>dd{margin-left:0;padding-left:3.2em;text-indent:-3.2em;list-style:none}.mw-parser-output .refbegin-100{font-size:100%}</style><div class="refbegin" style="">
<ul><li><cite class="citation conference">J M Ponte and W B Croft (1998). "A Language Modeling Approach to Information Retrieval". <i>Research and Development in Information Retrieval</i>. pp.&#160;275–281. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.117.4237">10.1.1.117.4237</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=A+Language+Modeling+Approach+to+Information+Retrieval&amp;rft.btitle=Research+and+Development+in+Information+Retrieval&amp;rft.pages=275-281&amp;rft.date=1998&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.117.4237&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALanguage+model" class="Z3988"></span><span class="cs1-maint citation-comment">CS1 maint: Uses authors parameter (<a href="/wiki/Category:CS1_maint:_Uses_authors_parameter" title="Category:CS1 maint: Uses authors parameter">link</a>)</span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r879151008"/></li>
<li><cite class="citation conference">F Song and W B Croft (1999). "A General Language Model for Information Retrieval". <i>Research and Development in Information Retrieval</i>. pp.&#160;279–280. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.21.6467">10.1.1.21.6467</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=A+General+Language+Model+for+Information+Retrieval&amp;rft.btitle=Research+and+Development+in+Information+Retrieval&amp;rft.pages=279-280&amp;rft.date=1999&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.21.6467&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALanguage+model" class="Z3988"></span><span class="cs1-maint citation-comment">CS1 maint: Uses authors parameter (<a href="/wiki/Category:CS1_maint:_Uses_authors_parameter" title="Category:CS1 maint: Uses authors parameter">link</a>)</span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r879151008"/></li>
<li><cite class="citation techreport">Chen, Stanley; Joshua Goodman (1998). <i>An Empirical Study of Smoothing Techniques for Language Modeling</i> (Technical report). Harvard University. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.131.5458">10.1.1.131.5458</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=report&amp;rft.btitle=An+Empirical+Study+of+Smoothing+Techniques+for+Language+Modeling&amp;rft.pub=Harvard+University&amp;rft.date=1998&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.131.5458&amp;rft.aulast=Chen&amp;rft.aufirst=Stanley&amp;rft.au=Joshua+Goodman&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALanguage+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r879151008"/></li></ul>
</div>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Language_model&amp;action=edit&amp;section=12" title="Edit section: External links">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a rel="nofollow" class="external text" href="http://www.cs.columbia.edu/~mcollins/">Lecture notes on language models, parsing and machine translation with PCFG, CRF, MaxEnt, MEMM, EM, GLM, HMM by Michael Collins(Columbia University)</a></li>
<li><a rel="nofollow" class="external text" href="http://www-lium.univ-lemans.fr/cslm">CSLM</a> – Free toolkit for <a href="/wiki/Feedforward_neural_network" title="Feedforward neural network">feedforward neural</a> language models</li>
<li><a rel="nofollow" class="external text" href="https://github.com/jnory/DALM">DALM</a> – Fast, Free software for language model queries</li>
<li><a rel="nofollow" class="external text" href="http://sourceforge.net/projects/irstlm">IRSTLM</a> – Free software for language modeling</li>
<li><a rel="nofollow" class="external text" href="http://www.phontron.com/kylm">Kylm</a> (Kyoto Language Modeling Toolkit) – Free language modeling toolkit in Java</li>
<li><a rel="nofollow" class="external text" href="http://kheafield.com/code/kenlm">KenLM</a> – Fast, Free software for language modeling</li>
<li><a rel="nofollow" class="external text" href="https://lmsharp.codeplex.com/">LMSharp</a> – Free language model toolkit for <a href="/wiki/Kneser%E2%80%93Ney_smoothing" title="Kneser–Ney smoothing">Kneser–Ney-smoothed</a> <i>n</i>-gram models and <a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">recurrent neural network</a> models</li>
<li><a rel="nofollow" class="external text" href="https://code.google.com/p/mitlm">MITLM</a> – MIT Language Modeling toolkit.  Free software</li>
<li><a rel="nofollow" class="external text" href="http://nlg.isi.edu/software/nplm">NPLM</a> – Free toolkit for <a href="/wiki/Feedforward_neural_network" title="Feedforward neural network">feedforward neural</a> language models</li>
<li><a rel="nofollow" class="external text" href="http://openfst.cs.nyu.edu/twiki/bin/view/GRM/NGramLibrary">OpenGrm NGram</a> library – Free software for language modeling.  Built on <a href="/w/index.php?title=OpenFst&amp;action=edit&amp;redlink=1" class="new" title="OpenFst (page does not exist)">OpenFst</a>.</li>
<li><a rel="nofollow" class="external text" href="https://github.com/pauldb89/OxLM">OxLM</a> – Free toolkit for <a href="/wiki/Feedforward_neural_network" title="Feedforward neural network">feedforward neural</a> language models</li>
<li><a rel="nofollow" class="external text" href="http://sifaka.cs.uiuc.edu/~ylv2/pub/plm/plm.htm">Positional Language Model</a></li>
<li><a rel="nofollow" class="external text" href="http://sourceforge.net/projects/randlm">RandLM</a> – Free software for <a href="/w/index.php?title=Randomised_language_modeling&amp;action=edit&amp;redlink=1" class="new" title="Randomised language modeling (page does not exist)">randomised language modeling</a></li>
<li><a rel="nofollow" class="external text" href="http://rnnlm.org">RNNLM</a> – Free <a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">recurrent neural network</a> language model toolkit</li>
<li><a rel="nofollow" class="external text" href="https://web.archive.org/web/20120302151523/http://www-speech.sri.com/projects/srilm/">SRILM</a> – Proprietary software for language modeling</li>
<li><a rel="nofollow" class="external text" href="https://vsiivola.github.io/variKN">VariKN</a> – Free software for creating, growing and pruning Kneser-Ney smoothed <i>n</i>-gram models.</li>
<li><a rel="nofollow" class="external text" href="http://www.keithv.com/software/csr/">Language models trained on newswire data</a></li>
<li><a rel="nofollow" class="external text" href="http://www.ngram.info/">Web Page NGram Viewer</a></li></ul>

<!-- 
NewPP limit report
Parsed by mw1249
Cached time: 20190211200410
Cache expiry: 2073600
Dynamic content: false
CPU time usage: 0.360 seconds
Real time usage: 0.525 seconds
Preprocessor visited node count: 1516/1000000
Preprocessor generated node count: 0/1500000
Post‐expand include size: 22339/2097152 bytes
Template argument size: 2129/2097152 bytes
Highest expansion depth: 13/40
Expensive parser function count: 5/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 26867/5000000 bytes
Number of Wikibase entities loaded: 2/400
Lua time usage: 0.161/10.000 seconds
Lua memory usage: 4.29 MB/50 MB
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  387.969      1 -total
 46.95%  182.154      2 Template:Reflist
 18.57%   72.050      2 Template:Cite_web
 15.02%   58.281      2 Template:Fix
 14.40%   55.856      1 Template:Examples
  9.53%   36.959      5 Template:Category_handler
  9.39%   36.430      1 Template:ISBN
  8.67%   33.646      3 Template:Cite_conference
  7.70%   29.868      1 Template:Cite_arXiv
  7.41%   28.747      1 Template:Main
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:1911810-0!canonical!math=5 and timestamp 20190211200409 and revision id 880427857
 -->
</div><noscript><img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript></div>					<div class="printfooter">
						Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Language_model&amp;oldid=880427857">https://en.wikipedia.org/w/index.php?title=Language_model&amp;oldid=880427857</a>"					</div>
				<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Language_modeling" title="Category:Language modeling">Language modeling</a></li><li><a href="/wiki/Category:Statistical_natural_language_processing" title="Category:Statistical natural language processing">Statistical natural language processing</a></li><li><a href="/wiki/Category:Markov_models" title="Category:Markov models">Markov models</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:All_articles_needing_examples" title="Category:All articles needing examples">All articles needing examples</a></li><li><a href="/wiki/Category:Articles_needing_examples_from_December_2017" title="Category:Articles needing examples from December 2017">Articles needing examples from December 2017</a></li><li><a href="/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_December_2017" title="Category:Articles with unsourced statements from December 2017">Articles with unsourced statements from December 2017</a></li><li><a href="/wiki/Category:Wikipedia_articles_needing_clarification_from_November_2018" title="Category:Wikipedia articles needing clarification from November 2018">Wikipedia articles needing clarification from November 2018</a></li><li><a href="/wiki/Category:CS1_maint:_Uses_authors_parameter" title="Category:CS1 maint: Uses authors parameter">CS1 maint: Uses authors parameter</a></li></ul></div></div>				<div class="visualClear"></div>
							</div>
		</div>
		<div id="mw-navigation">
			<h2>Navigation menu</h2>
			<div id="mw-head">
									<div id="p-personal" role="navigation" aria-labelledby="p-personal-label">
						<h3 id="p-personal-label">Personal tools</h3>
						<ul>
							<li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n">Talk</a></li><li id="pt-anoncontribs"><a href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Language+model" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Language+model" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o">Log in</a></li>						</ul>
					</div>
									<div id="left-navigation">
										<div id="p-namespaces" role="navigation" class="vectorTabs" aria-labelledby="p-namespaces-label">
						<h3 id="p-namespaces-label">Namespaces</h3>
						<ul>
							<li id="ca-nstab-main" class="selected"><span><a href="/wiki/Language_model" title="View the content page [c]" accesskey="c">Article</a></span></li><li id="ca-talk"><span><a href="/wiki/Talk:Language_model" rel="discussion" title="Discussion about the content page [t]" accesskey="t">Talk</a></span></li>						</ul>
					</div>
										<div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">
												<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-variants-label" />
						<h3 id="p-variants-label">
							<span>Variants</span>
						</h3>
						<ul class="menu">
													</ul>
					</div>
									</div>
				<div id="right-navigation">
										<div id="p-views" role="navigation" class="vectorTabs" aria-labelledby="p-views-label">
						<h3 id="p-views-label">Views</h3>
						<ul>
							<li id="ca-view" class="collapsible selected"><span><a href="/wiki/Language_model">Read</a></span></li><li id="ca-edit" class="collapsible"><span><a href="/w/index.php?title=Language_model&amp;action=edit" title="Edit this page [e]" accesskey="e">Edit</a></span></li><li id="ca-history" class="collapsible"><span><a href="/w/index.php?title=Language_model&amp;action=history" title="Past revisions of this page [h]" accesskey="h">View history</a></span></li>						</ul>
					</div>
										<div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label">
						<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-cactions-label" />
						<h3 id="p-cactions-label"><span>More</span></h3>
						<ul class="menu">
													</ul>
					</div>
										<div id="p-search" role="search">
						<h3>
							<label for="searchInput">Search</label>
						</h3>
						<form action="/w/index.php" id="searchform">
							<div id="simpleSearch">
								<input type="search" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" accesskey="f" id="searchInput"/><input type="hidden" value="Special:Search" name="title"/><input type="submit" name="fulltext" value="Search" title="Search Wikipedia for this text" id="mw-searchButton" class="searchButton mw-fallbackSearchButton"/><input type="submit" name="go" value="Go" title="Go to a page with this exact name if it exists" id="searchButton" class="searchButton"/>							</div>
						</form>
					</div>
									</div>
			</div>
			<div id="mw-panel">
				<div id="p-logo" role="banner"><a class="mw-wiki-logo" href="/wiki/Main_Page" title="Visit the main page"></a></div>
						<div class="portal" role="navigation" id="p-navigation" aria-labelledby="p-navigation-label">
			<h3 id="p-navigation-label">Navigation</h3>
			<div class="body">
								<ul>
					<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li><li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikipedia store">Wikipedia store</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-interaction" aria-labelledby="p-interaction-label">
			<h3 id="p-interaction-label">Interaction</h3>
			<div class="body">
								<ul>
					<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-tb" aria-labelledby="p-tb-label">
			<h3 id="p-tb-label">Tools</h3>
			<div class="body">
								<ul>
					<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Language_model" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li><li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Language_model" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li><li id="t-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u">Upload file</a></li><li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Language_model&amp;oldid=880427857" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Language_model&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q3621696" title="Link to connected data repository item [g]" accesskey="g">Wikidata item</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Language_model&amp;id=880427857" title="Information on how to cite this page">Cite this page</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-coll-print_export" aria-labelledby="p-coll-print_export-label">
			<h3 id="p-coll-print_export-label">Print/export</h3>
			<div class="body">
								<ul>
					<li id="coll-create_a_book"><a href="/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Language+model">Create a book</a></li><li id="coll-download-as-rdf2latex"><a href="/w/index.php?title=Special:ElectronPdf&amp;page=Language+model&amp;action=show-download-screen">Download as PDF</a></li><li id="t-print"><a href="/w/index.php?title=Language_model&amp;printable=yes" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-lang" aria-labelledby="p-lang-label">
			<h3 id="p-lang-label">Languages</h3>
			<div class="body">
								<ul>
					<li class="interlanguage-link interwiki-ar"><a href="https://ar.wikipedia.org/wiki/%D9%82%D8%A7%D9%84%D8%A8_%D8%A7%D9%84%D9%84%D8%BA%D8%A9" title="قالب اللغة – Arabic" lang="ar" hreflang="ar" class="interlanguage-link-target">العربية</a></li><li class="interlanguage-link interwiki-bg"><a href="https://bg.wikipedia.org/wiki/%D0%95%D0%B7%D0%B8%D0%BA%D0%BE%D0%B2_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB" title="Езиков модел – Bulgarian" lang="bg" hreflang="bg" class="interlanguage-link-target">Български</a></li><li class="interlanguage-link interwiki-ca"><a href="https://ca.wikipedia.org/wiki/Model_de_llenguatge" title="Model de llenguatge – Catalan" lang="ca" hreflang="ca" class="interlanguage-link-target">Català</a></li><li class="interlanguage-link interwiki-es"><a href="https://es.wikipedia.org/wiki/Modelaci%C3%B3n_del_lenguaje" title="Modelación del lenguaje – Spanish" lang="es" hreflang="es" class="interlanguage-link-target">Español</a></li><li class="interlanguage-link interwiki-nn"><a href="https://nn.wikipedia.org/wiki/Spr%C3%A5kmodell" title="Språkmodell – Norwegian Nynorsk" lang="nn" hreflang="nn" class="interlanguage-link-target">Norsk nynorsk</a></li><li class="interlanguage-link interwiki-zh"><a href="https://zh.wikipedia.org/wiki/%E8%AA%9E%E8%A8%80%E6%A8%A1%E5%9E%8B" title="語言模型 – Chinese" lang="zh" hreflang="zh" class="interlanguage-link-target">中文</a></li>				</ul>
				<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q3621696#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>			</div>
		</div>
				</div>
		</div>
				<div id="footer" role="contentinfo">
						<ul id="footer-info">
								<li id="footer-info-lastmod"> This page was last edited on 27 January 2019, at 10:41<span class="anonymous-show">&#160;(UTC)</span>.</li>
								<li id="footer-info-copyright">Text is available under the <a rel="license" href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="//creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
							</ul>
						<ul id="footer-places">
								<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>
								<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
								<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
								<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
								<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
								<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
								<li id="footer-places-mobileview"><a href="//en.m.wikipedia.org/w/index.php?title=Language_model&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
							</ul>
										<ul id="footer-icons" class="noprint">
										<li id="footer-copyrightico">
						<a href="https://wikimediafoundation.org/"><img src="/static/images/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation"/></a>					</li>
										<li id="footer-poweredbyico">
						<a href="//www.mediawiki.org/"><img src="/static/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88" height="31"/></a>					</li>
									</ul>
						<div style="clear: both;"></div>
		</div>
		
<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.360","walltime":"0.525","ppvisitednodes":{"value":1516,"limit":1000000},"ppgeneratednodes":{"value":0,"limit":1500000},"postexpandincludesize":{"value":22339,"limit":2097152},"templateargumentsize":{"value":2129,"limit":2097152},"expansiondepth":{"value":13,"limit":40},"expensivefunctioncount":{"value":5,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":26867,"limit":5000000},"entityaccesscount":{"value":2,"limit":400},"timingprofile":["100.00%  387.969      1 -total"," 46.95%  182.154      2 Template:Reflist"," 18.57%   72.050      2 Template:Cite_web"," 15.02%   58.281      2 Template:Fix"," 14.40%   55.856      1 Template:Examples","  9.53%   36.959      5 Template:Category_handler","  9.39%   36.430      1 Template:ISBN","  8.67%   33.646      3 Template:Cite_conference","  7.70%   29.868      1 Template:Cite_arXiv","  7.41%   28.747      1 Template:Main"]},"scribunto":{"limitreport-timeusage":{"value":"0.161","limit":"10.000"},"limitreport-memusage":{"value":4494695,"limit":52428800}},"cachereport":{"origin":"mw1249","timestamp":"20190211200410","ttl":2073600,"transientcontent":false}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"Language model","url":"https:\/\/en.wikipedia.org\/wiki\/Language_model","sameAs":"http:\/\/www.wikidata.org\/entity\/Q3621696","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q3621696","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2005-05-19T22:47:52Z","dateModified":"2019-01-27T10:41:23Z","headline":"statistical model of structure of language"}</script>
<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":104,"wgHostname":"mw1273"});});</script>
	</body>
</html>
